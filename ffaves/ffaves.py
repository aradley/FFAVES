### In this file we provide all the code necessary to run FFAVES and ESFW, as described in out publication,
# "Functional feature selection reveals the inner cell mass in human pre-implantation embryo single cell RNA sequencing data".

### Dependencies ###

import numpy as np
from functools import partial 
import multiprocess
import copy
from scipy.stats import halfnorm, zscore

### Dependencies ###

### Here we have the FFAVES wrapper function that executes all the steps of FFAVES. ###
## Binarised_Input_Matrix: The discretised matrix where rows are samples and columns are features. Each sample for each feature
# should be represented by either a 0 or a 1. It does not matter if your 0's or 1's represent the minority state for each feature,
# FFAVES will work it out.
## Min_Clust_Size: The minimum number of samples a feature can have in the minority state before it is ignored.
## Divergences_Significance_Cut_Off: The confidence interval used when identifying statistically significant false positve or false
# negative data points.
## Use_Cores: The number of cores used. When equal to -1, will use one less than the number of cores on your machine.
## Max_Num_Cycles: The maximum number of cycles of FFAVES that will be run before terminating.
## Tolerance: The tolerance for the percentage change of identified false positive and false negative datapoints between cycles.
# If within the tolerance for 3 cycles in the row, FFAVES will terminate earlier than Max_Num_Cycles.
## Auto_Save: If 1, objected generated by FFAVES will be saved after each cycle. If any other number, object will not be automatically saved.
def FFAVES(Binarised_Input_Matrix, Min_Clust_Size = 5, Divergences_Significance_Cut_Off = 0.99, Use_Cores= -1, Max_Num_Cycles = 15, Tolerance = 0.1, Auto_Save = 1):
    # Set number of cores to use
    Cores_Available = multiprocess.cpu_count()
    if Use_Cores == -1:
        Use_Cores = Cores_Available - 1 # -1 Is an arbitrary buffer of idle cores that I set.
        if Use_Cores < 1:
            Use_Cores = 1
    print("Cores Avaiblable: " + str(Cores_Available))
    print("Cores Used: " + str(Use_Cores))
    # Remove genes below Min_Clust_Size
    Keep_Features = np.where(np.sum(Binarised_Input_Matrix,axis=0) >= Min_Clust_Size)[0]
    Ignore = np.where(np.sum(Binarised_Input_Matrix,axis=0) >= (Binarised_Input_Matrix.shape[0]-Min_Clust_Size))[0] 
    Keep_Features = np.delete(Keep_Features,np.where(np.isin(Keep_Features,Ignore))[0])
    if Keep_Features.shape[0] < Binarised_Input_Matrix.shape[1]:
        print("Ignoring " + str(Binarised_Input_Matrix.shape[1]-Keep_Features.shape[0]) + " features which are below the Min_Clust_Threshold")
        Binarised_Input_Matrix = Binarised_Input_Matrix[:,Keep_Features]
    # Define data dimensions
    #global Cell_Cardinality
    Cell_Cardinality = Binarised_Input_Matrix.shape[0]
    #global Gene_Cardinality
    Gene_Cardinality = Binarised_Input_Matrix.shape[1]
    # Convert Binarised_Input_Matrix into Minority_Group_Matrix
    Permutables, Switch_State_Inidicies = Find_Permutations(Binarised_Input_Matrix,Cell_Cardinality)
    Binarised_Input_Matrix[:,Switch_State_Inidicies] = (Binarised_Input_Matrix[:,Switch_State_Inidicies] * -1) + 1  
    # Set up Minority_Group_Matrix
    global Minority_Group_Matrix
    # Track what cycle FFAVES is on.
    global Imputation_Cycle
    Imputation_Cycle = 1
    print("Number of cells: " + str(Cell_Cardinality))
    print("Number of genes: " + str(Gene_Cardinality))
    Track_Percentage_Imputation = np.zeros((3,Max_Num_Cycles+1))
    Track_Imputation_Steps = np.empty(((Max_Num_Cycles + 1),),dtype="object")
    Track_Imputation_Steps[...]=[([[]]*3) for _ in range((Max_Num_Cycles + 1))]  
    # Set up tolerance tracker
    Covergence_Counter = 0
    All_Impute_Inds = (np.array([]).astype("i"),np.array([]).astype("i"))
    while Imputation_Cycle <= Max_Num_Cycles and Covergence_Counter < 3:
        if Imputation_Cycle > 1:
            print("Percentage of original data suggested as Type 1 Error: " + str(np.round((Track_Imputation_Steps[Imputation_Cycle-1][0][0].shape[0]/(Binarised_Input_Matrix.shape[0]*Binarised_Input_Matrix.shape[1]))*100,2)) + "%")
            print("Percentage of original data suggested as Type 2 Error: " + str(np.round((Track_Imputation_Steps[Imputation_Cycle-1][2][0].shape[0]/(Binarised_Input_Matrix.shape[0]*Binarised_Input_Matrix.shape[1]))*100,2)) + "%")   
        # Initiate Cycle_Imputation_Steps
        Cycle_Imputation_Steps = [[]] * 3
        # Initiate State_Inversions
        State_Inversions = np.zeros(Gene_Cardinality)
        print("Cycle Number " + str(Imputation_Cycle))         
        # Convert suggested imputation points from previous cycle to correct state.
        Minority_Group_Matrix = copy.copy(Binarised_Input_Matrix)
        Minority_Group_Matrix[All_Impute_Inds] = (Minority_Group_Matrix[All_Impute_Inds] - 1) * -1
        ### Step 1 of FFAVES is to identify and temporarily remove spurious Minority Group expression states
        with np.errstate(divide='ignore',invalid='ignore'):
            Step_1_Type_1_Error_Inds, Switch_State_Inidicies_1 = FFAVES_Step_1(Min_Clust_Size,Divergences_Significance_Cut_Off,Use_Cores,Cell_Cardinality,Gene_Cardinality)
        # Switch Minority_Group_Matrix Inds
        Minority_Group_Matrix[Step_1_Type_1_Error_Inds] = (Minority_Group_Matrix[Step_1_Type_1_Error_Inds] - 1) * -1
        #
        State_Inversions[Switch_State_Inidicies_1] = (State_Inversions[Switch_State_Inidicies_1] * -1) + 1
        State_Inversion_Inds = np.where(State_Inversions == 1)[0]
        Flat_All_Impute_Inds = np.ravel_multi_index(All_Impute_Inds, Binarised_Input_Matrix.shape)
        Flat_Step_1_Inds = np.ravel_multi_index(Step_1_Type_1_Error_Inds, Binarised_Input_Matrix.shape)
        Void_Type_1_Errors = np.where(np.isin(Step_1_Type_1_Error_Inds[1],State_Inversion_Inds)==0)[0]
        Impute_Type_1_Errors = np.where(np.isin(Step_1_Type_1_Error_Inds[1],State_Inversion_Inds)==1)[0]
        if Void_Type_1_Errors.shape[0] > 0:
            Void_Type_1_Errors = np.ravel_multi_index((Step_1_Type_1_Error_Inds[0][Void_Type_1_Errors],Step_1_Type_1_Error_Inds[1][Void_Type_1_Errors]), Binarised_Input_Matrix.shape)
            Ignore_Imputations = np.where(np.isin(Flat_All_Impute_Inds,Flat_Step_1_Inds))[0]
            Flat_All_Impute_Inds = np.delete(Flat_All_Impute_Inds,Ignore_Imputations)       
        All_Impute_Inds = np.unravel_index(Flat_All_Impute_Inds,Binarised_Input_Matrix.shape)
        ### Don't use below because removal of false positives should be temporary
        #if Impute_Type_1_Errors.shape[0] > 0:
        #    Impute_Type_1_Errors = np.ravel_multi_index((Step_1_Type_1_Error_Inds[0][Impute_Type_1_Errors],Step_1_Type_1_Error_Inds[1][Impute_Type_1_Errors]), Binarised_Input_Matrix.shape)
        #    Flat_All_Impute_Inds = np.unique(np.append(Flat_All_Impute_Inds,Impute_Type_1_Errors))
        ### Step 2 of FFAVES is to identify which majority states points are spurious
        with np.errstate(divide='ignore',invalid='ignore'):
            Step_2_Type_2_Error_Inds, Switch_State_Inidicies_2, Average_Imputed_Divergence = FFAVES_Step_2(Min_Clust_Size,Divergences_Significance_Cut_Off,Use_Cores,Cell_Cardinality,Gene_Cardinality)        
        # Switch Minority_Group_Matrix Inds
        Minority_Group_Matrix[Step_2_Type_2_Error_Inds] = (Minority_Group_Matrix[Step_2_Type_2_Error_Inds] - 1) * -1
        #
        State_Inversions[Switch_State_Inidicies_2] = (State_Inversions[Switch_State_Inidicies_2] * -1) + 1
        State_Inversion_Inds = np.where(State_Inversions == 1)[0]
        Flat_All_Impute_Inds = np.ravel_multi_index(All_Impute_Inds, Binarised_Input_Matrix.shape)
        Flat_Step_2_Inds = np.ravel_multi_index(Step_2_Type_2_Error_Inds, Binarised_Input_Matrix.shape)
        Void_Type_2_Errors = np.where(np.isin(Step_2_Type_2_Error_Inds[1],State_Inversion_Inds)==1)[0]
        Impute_Type_2_Errors = np.where(np.isin(Step_2_Type_2_Error_Inds[1],State_Inversion_Inds)==0)[0]
        if Void_Type_2_Errors.shape[0] > 0:
            Void_Type_2_Errors = np.ravel_multi_index((Step_2_Type_2_Error_Inds[0][Void_Type_2_Errors],Step_2_Type_2_Error_Inds[1][Void_Type_2_Errors]), Binarised_Input_Matrix.shape)
            Ignore_Imputations = np.where(np.isin(Flat_All_Impute_Inds,Flat_Step_2_Inds))[0]
            Flat_All_Impute_Inds = np.delete(Flat_All_Impute_Inds,Ignore_Imputations) 
        if Impute_Type_2_Errors.shape[0] > 0:
            Impute_Type_2_Errors = np.ravel_multi_index((Step_2_Type_2_Error_Inds[0][Impute_Type_2_Errors],Step_2_Type_2_Error_Inds[1][Impute_Type_2_Errors]), Binarised_Input_Matrix.shape)
            Flat_All_Impute_Inds = np.unique(np.append(Flat_All_Impute_Inds,Impute_Type_2_Errors))
        All_Impute_Inds = np.unravel_index(Flat_All_Impute_Inds,Binarised_Input_Matrix.shape)
        ### Step 3 of FFAVES is to NULL any suggested False Negative imputations that are spurious/conflicting
        with np.errstate(divide='ignore',invalid='ignore'):
            Step_3_Type_1_Error_Inds, Switch_State_Inidicies_3 = FFAVES_Step_3(Min_Clust_Size,Divergences_Significance_Cut_Off,Use_Cores,Cell_Cardinality,Gene_Cardinality)
        # Void conflicting suggested false negative imputations
        Flat_All_Impute_Inds = np.ravel_multi_index(All_Impute_Inds, Binarised_Input_Matrix.shape)
        Flat_Step_3_Inds = np.ravel_multi_index(Step_3_Type_1_Error_Inds, Binarised_Input_Matrix.shape)
        Void_All_Impute_Inds = np.where(np.isin(Flat_All_Impute_Inds,Flat_Step_3_Inds)==1)[0]
        Flat_All_Impute_Inds = np.delete(Flat_All_Impute_Inds,Void_All_Impute_Inds)
        All_Impute_Inds = np.unravel_index(Flat_All_Impute_Inds,Binarised_Input_Matrix.shape)        
        ### Track Erroneous points
        Cycle_Imputation_Steps[0] = Step_1_Type_1_Error_Inds
        Cycle_Imputation_Steps[1] = Step_2_Type_2_Error_Inds
        Cycle_Imputation_Steps[2] = All_Impute_Inds
        Track_Imputation_Steps[Imputation_Cycle] = Cycle_Imputation_Steps
        print("Finished")
        Track_Percentage_Imputation[0,Imputation_Cycle] = (Track_Imputation_Steps[Imputation_Cycle][0][0].shape[0]/(Binarised_Input_Matrix.shape[0]*Binarised_Input_Matrix.shape[1]))*100
        Track_Percentage_Imputation[1,Imputation_Cycle] = (Track_Imputation_Steps[Imputation_Cycle][2][0].shape[0]/(Binarised_Input_Matrix.shape[0]*Binarised_Input_Matrix.shape[1]))*100
        Track_Percentage_Imputation[2,Imputation_Cycle] = Average_Imputed_Divergence
        if Imputation_Cycle < Max_Num_Cycles:
            Imputed_Difference = np.sum(np.absolute(Track_Percentage_Imputation[0:2,Imputation_Cycle] - Track_Percentage_Imputation[0:2,Imputation_Cycle-1]))
            if Imputed_Difference <= Tolerance:
                Covergence_Counter = Covergence_Counter + 1
            else:
                Covergence_Counter = 0
        Imputation_Cycle = Imputation_Cycle + 1
    if Imputation_Cycle < Max_Num_Cycles:
        Track_Imputation_Steps = Track_Imputation_Steps[0:Imputation_Cycle]
        Track_Percentage_Imputation = Track_Percentage_Imputation[:,0:Imputation_Cycle]
    # Re-align inds to original data prior to Min_Clust_Size subsetting
    for i in np.arange(1,Track_Imputation_Steps.shape[0]):
        Track_Imputation_Steps[i][0] = (Track_Imputation_Steps[i][0][0],Keep_Features[Track_Imputation_Steps[i][0][1]])
        Track_Imputation_Steps[i][1] = (Track_Imputation_Steps[i][1][0],Keep_Features[Track_Imputation_Steps[i][1][1]])
        Track_Imputation_Steps[i][2] = (Track_Imputation_Steps[i][2][0],Keep_Features[Track_Imputation_Steps[i][2][1]])
    if Auto_Save == 1:
        np.save("Track_Imputation_Steps.npy",Track_Imputation_Steps)
        np.save("Track_Percentage_Imputation.npy",Track_Percentage_Imputation)
    return Track_Imputation_Steps, Track_Percentage_Imputation

### Wrapper functions for each of the 3 main steps of FFAVES, which are described in our publication.

def FFAVES_Step_1(Min_Clust_Size,Divergences_Significance_Cut_Off,Use_Cores,Cell_Cardinality,Gene_Cardinality):
    print("Step 1: Quantifying Type 1 Error for each data point.")
    print("Identifying Sort Info for calculations.") 
    # Create Minority_Group_Matrix objects, Permutables and Switch_State_Inidicies objects.
    Permutables, Switch_State_Inidicies = Find_Permutations(Minority_Group_Matrix,Cell_Cardinality)
    # Switch Minority/Majority states to 0/1 where necessary.
    Minority_Group_Matrix[:,Switch_State_Inidicies] = (Minority_Group_Matrix[:,Switch_State_Inidicies] * -1) + 1  
    # Calculate minority group overlap matrix 
    Reference_Gene_Minority_Group_Overlaps = Parallel_Find_Minority_Group_Overlaps(Use_Cores,Gene_Cardinality)
    Permutables[Permutables < Min_Clust_Size] = np.nan
    print("Calculating Divergence Matrix.")
    Type_1_Error_Divergences = Parallel_Calculate_Dependent_Divergence_Per_Cell("1",Cell_Cardinality,Gene_Cardinality,Permutables,Reference_Gene_Minority_Group_Overlaps,Use_Cores)
    Type_1_Error_Inds_1 = Extract_Divergence_Info("1", Type_1_Error_Divergences, Divergences_Significance_Cut_Off)
    Type_1_Error_Inds_1 = np.unravel_index(Type_1_Error_Inds_1,Minority_Group_Matrix.shape)
    return Type_1_Error_Inds_1, Switch_State_Inidicies


def FFAVES_Step_2(Min_Clust_Size,Divergences_Significance_Cut_Off,Use_Cores,Cell_Cardinality,Gene_Cardinality): 
    print("Step 2: Quantifying Type 2 Error for each data point.")
    print("Identifying Sort Info for calculations.")
    # Create Minority_Group_Matrix objects, Permutables and Switch_State_Inidicies objects.
    Permutables, Switch_State_Inidicies = Find_Permutations(Minority_Group_Matrix,Cell_Cardinality)
    # Switch Minority/Majority states to 0/1 where necessary. 
    Minority_Group_Matrix[:,Switch_State_Inidicies] = (Minority_Group_Matrix[:,Switch_State_Inidicies] * -1) + 1
    # Calculate minority group overlap matrix
    Reference_Gene_Minority_Group_Overlaps = Parallel_Find_Minority_Group_Overlaps(Use_Cores,Gene_Cardinality)
    Permutables[Permutables < Min_Clust_Size] = np.nan
    print("Calculating Divergence Matrix.")   
    Type_2_Error_Divergences = Parallel_Calculate_Dependent_Divergence_Per_Cell("2",Cell_Cardinality,Gene_Cardinality,Permutables,Reference_Gene_Minority_Group_Overlaps,Use_Cores)
    Type_2_Error_Inds = Extract_Divergence_Info("2", Type_2_Error_Divergences, Divergences_Significance_Cut_Off)
    Type_2_Error_Inds = np.unravel_index(Type_2_Error_Inds,Minority_Group_Matrix.shape)
    Average_Imputed_Divergence = np.mean(Type_2_Error_Divergences[Type_2_Error_Inds])
    return Type_2_Error_Inds, Switch_State_Inidicies, Average_Imputed_Divergence


def FFAVES_Step_3(Min_Clust_Size,Divergences_Significance_Cut_Off,Use_Cores,Cell_Cardinality,Gene_Cardinality):
    print("Step 3: Ignoring conflicting suggested imputations.")
    print("Identifying Sort Info for calculations.") 
    # Create Minority_Group_Matrix objects, Permutables and Switch_State_Inidicies objects.
    Permutables, Switch_State_Inidicies = Find_Permutations(Minority_Group_Matrix,Cell_Cardinality)
    # Switch Minority/Majority states to 0/1 where necessary.
    Minority_Group_Matrix[:,Switch_State_Inidicies] = (Minority_Group_Matrix[:,Switch_State_Inidicies] * -1) + 1  
    # Calculate minority group overlap matrix 
    Reference_Gene_Minority_Group_Overlaps = Parallel_Find_Minority_Group_Overlaps(Use_Cores,Gene_Cardinality)
    Permutables[Permutables < Min_Clust_Size] = np.nan
    print("Calculating Divergence Matrix.")
    Type_1_Error_Divergences = Parallel_Calculate_Dependent_Divergence_Per_Cell("1",Cell_Cardinality,Gene_Cardinality,Permutables,Reference_Gene_Minority_Group_Overlaps,Use_Cores)
    Type_1_Error_Inds_1 = Extract_Divergence_Info("1", Type_1_Error_Divergences, Divergences_Significance_Cut_Off)
    Type_1_Error_Inds_1 = np.unravel_index(Type_1_Error_Inds_1,Minority_Group_Matrix.shape)
    return Type_1_Error_Inds_1, Switch_State_Inidicies


### Here we have all of FFAVES subfunctions that are needed to calculate ES scores. ###

## Find the partition basis for each reference feature.
def Find_Permutations(Minority_Group_Matrix,Cell_Cardinality):
    Permutables = np.sum(Minority_Group_Matrix,axis=0).astype("f")
    Switch_State_Inidicies = np.where(Permutables >= (Cell_Cardinality/2))[0]
    Permutables[Switch_State_Inidicies] = Cell_Cardinality - Permutables[Switch_State_Inidicies]  
    return Permutables, Switch_State_Inidicies

## Depending on whether we are looking for sample divergences or feature weighting divergenes, we extract different information.
def Extract_Divergence_Info(Error_Type, Error_Divergences, Divergences_Significance_Cut_Off):
    # Use half normal distribution of normalised divergent points to suggest which points should be re-evaluated
    if Error_Type != "2":
        Use_Inds = np.where(Minority_Group_Matrix != 0)
    else:
        Use_Inds = np.where(Minority_Group_Matrix == 0)
    Divergences = Error_Divergences[Use_Inds]
    ## Get zscores for observed divergences
    # Calculate the standard deviation for the unfolded normal distribution
    std = np.std(np.append(Divergences,-Divergences))
    # Calculate z-scores with sample mean = 0 
    zscores = Divergences/std
    #zscores = zscores + np.absolute(np.min(zscores))
    # Identify points that diverge in a statistically significant way
    Pass_Threshold = np.where(halfnorm.cdf(zscores,loc=0) >= Divergences_Significance_Cut_Off)[0]
    Error_Inds = (Use_Inds[0][Pass_Threshold],Use_Inds[1][Pass_Threshold])
    Error_Inds = np.ravel_multi_index(Error_Inds, Minority_Group_Matrix.shape)
    return Error_Inds


### Find minority group overlapping inds for each feature. Calculating this now streamlines future calculations
def Parallel_Find_Minority_Group_Overlaps(Use_Cores,Gene_Cardinality):
    Inds = np.arange(Gene_Cardinality)
    pool = multiprocess.Pool(processes = Use_Cores)
    Result = pool.map(Find_Minority_Group_Overlaps, Inds)
    pool.close()
    pool.join()
    Reference_Gene_Minority_Group_Overlaps = np.asarray(Result)
    return Reference_Gene_Minority_Group_Overlaps

def Find_Minority_Group_Overlaps(Ind):
    # For each feature, identify how often its minority state samples overlap with the minority state samples of every other feature.
    Reference_Gene = Minority_Group_Matrix[:,Ind]
    Overlaps = np.dot(Reference_Gene,Minority_Group_Matrix)
    return Overlaps

## The parallelised function for calculating the divergences of each sample for each feature.
def Parallel_Calculate_Dependent_Divergence_Per_Cell(Error_Type,Cell_Cardinality,Gene_Cardinality,Permutables,Reference_Gene_Minority_Group_Overlaps,Use_Cores):
    Feature_Inds = np.arange(Gene_Cardinality)
    Pass_Info_To_Cores = np.concatenate((Feature_Inds.reshape(1,Feature_Inds.shape[0]),Reference_Gene_Minority_Group_Overlaps))
    Pass_Info_To_Cores = np.transpose(Pass_Info_To_Cores)
    # Parrallel calculate information gains matrix
    pool = multiprocess.Pool(processes = Use_Cores)
    Result = pool.map(partial(Calculate_Dependent_Divergence_Per_Cell,Error_Type=Error_Type,Cell_Cardinality=Cell_Cardinality,Permutables=Permutables), Pass_Info_To_Cores)
    pool.close()
    pool.join()
    if Error_Type != "2_Intentional_Error": # For ESFW
        Divergence_Matrix = np.stack(Result,axis=1)
    else:
        Divergence_Matrix = np.asarray(Result) # For FFAVES
    return Divergence_Matrix

### Calculate Divergence Matrix
def Calculate_Dependent_Divergence_Per_Cell(Pass_Info_To_Cores,Error_Type,Cell_Cardinality,Permutables):   
    # Extract which gene is the Fixed Feature (FF)
    Feature_Inds = int(Pass_Info_To_Cores[0])
    if np.isnan(Permutables[Feature_Inds]) == 0:
        # Extract the gene
        Gene_States = Minority_Group_Matrix[:,Feature_Inds]
        Fixed_Gene_Minority_States = np.where(Gene_States == 1)[0]
        Fixed_Gene_Majority_States = np.where(Gene_States == 0)[0]
        # Remove the FF ind from the data vector
        Reference_Gene_Minority_Group_Overlaps = np.delete(Pass_Info_To_Cores,0)
        if Error_Type == "1": # Caluclate divergences for scenarios where divergence indicates false positive data points.
            ##### Get Fixed Feature Gene Information #####
            # Extract the group 1 and group 2 cardinalities. Group 1 is always the minority group in this set up.
            Minority_Group_Cardinality = Permutables[Feature_Inds]
            Majority_Group_Cardinality = Cell_Cardinality - Minority_Group_Cardinality
            # Identify in which scenarios the inspected gene is the Reference Feature (RF) and which it is the Query Feature (QF)
            # Whenever the inspected gene minority state cardinality is smaller than any other gene's, it is the RF
            RF_Inds = np.where(Permutables >= Minority_Group_Cardinality)[0]
            # Whenever the inspected gene minority state cardinality is larger than any other gene's, it is the QF
            QF_Inds = np.where(Permutables < Minority_Group_Cardinality)[0]
            ##### Perform caclulations when the FF gene is the RF and SD = -1 #####
            # Maximum entropy of the system is identified from the derivative of the Entropy Sorting Equation (ESE)
            Max_Entropy_Permutation = (Minority_Group_Cardinality * Permutables[RF_Inds])/(Minority_Group_Cardinality + Majority_Group_Cardinality)
            # The maximum and minimum points of the ESE are identified from the boundaries of the ES parabola.
            Min_Entropy_ID_1 = np.zeros(RF_Inds.shape[0])
            Min_Entropy_ID_2 = copy.copy(Permutables[RF_Inds])
            Check_Fits_Group_1 = Minority_Group_Cardinality - Min_Entropy_ID_2
            # If the minority group of the QF is larger than the minority group of the RF then the boundary point is the cardinality of the RF minority group.
            Min_Entropy_ID_2[np.where(Check_Fits_Group_1 < 0)[0]] = Minority_Group_Cardinality
            # Split_Permute_Value is the overlap of minority states that we actually observe in the data.
            Split_Permute_Value = Reference_Gene_Minority_Group_Overlaps[RF_Inds]
            # Num_Divergent_Cell identifies how many samples in the RF/QF lead to observable divergence
            Num_Divergent_Cell = Minority_Group_Cardinality - Split_Permute_Value
            # Identify Split Direction (whether the observed arrangment is sorting towards the global minimum entropy or not.
            Sort_Out_Of_Inds = np.where((Split_Permute_Value - Max_Entropy_Permutation) < 0)[0]
            Split_Direction = -1
            Split_Permute_Entropies, Max_Permuation_Entropies, Minimum_Entropies = Calculate_Fixed_RF_Sort_Values(2,Split_Direction,Max_Entropy_Permutation[Sort_Out_Of_Inds],Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables[RF_Inds[Sort_Out_Of_Inds]],Min_Entropy_ID_1[Sort_Out_Of_Inds],Min_Entropy_ID_2[Sort_Out_Of_Inds],Split_Permute_Value[Sort_Out_Of_Inds])
            ## Calculate divergence
            Divergences = Split_Permute_Entropies - Minimum_Entropies
            # Find the average divergence for each cell that is diverging from the optimal sort.
            Dependent_Divergence_Per_Cell = Divergences / Split_Permute_Value[Sort_Out_Of_Inds]
            # Calculate how much divergence each cell would have if the RF/QF system was at the maximum entropy arrangment.
            Max_Num_Dependent_Divergence_Per_Cell = Min_Entropy_ID_2[Sort_Out_Of_Inds] - Max_Entropy_Permutation[Sort_Out_Of_Inds]
            Independence_Divergence_Per_Cell = (Max_Permuation_Entropies)/Max_Num_Dependent_Divergence_Per_Cell
            # Calculate the Error Potential according to ES hypothesis testing.
            Error_Potential = Dependent_Divergence_Per_Cell - Independence_Divergence_Per_Cell
            # Null/Ignore points that aren't usable.
            Error_Potential[np.isinf(Error_Potential)] = 0
            Error_Potential[np.isnan(Error_Potential)] = 0
            # Feature pairs with positive EPs provide evidence of erroneous datapoints.
            Positive_EP_Inds = np.where(Error_Potential > 0)[0]
            # Get Overlap Matrix
            Sort_Genes = Minority_Group_Matrix[np.ix_(Fixed_Gene_Minority_States,RF_Inds[Sort_Out_Of_Inds][Positive_EP_Inds])]
            Sort_Out_Of_Partitioning_Divergences = np.zeros(Cell_Cardinality)
            Sort_Out_Of_Partitioning_Divergences[Fixed_Gene_Minority_States] = np.dot(Sort_Genes,Error_Potential[Positive_EP_Inds])        
            ##### Perform caclulations when the FF gene is the RF and SD = 1 #####
            # Maximum entropy of the system is identified from the derivative of the Entropy Sorting Equation (ESE)
            Max_Entropy_Permutation = (Minority_Group_Cardinality * Permutables[RF_Inds])/(Minority_Group_Cardinality + Majority_Group_Cardinality)
            # The maximum and minimum points of the ESE are identified from the boundaries of the ESE curve.
            Min_Entropy_ID_1 = np.zeros(RF_Inds.shape[0])
            Min_Entropy_ID_2 = copy.copy(Permutables[RF_Inds])
            Check_Fits_Group_1 = Minority_Group_Cardinality - Min_Entropy_ID_2
            # If the minority group of the QF is larger than the minority group of the RF then the boundary point is the cardinality of the RF minority group.
            Min_Entropy_ID_2[np.where(Check_Fits_Group_1 < 0)[0]] = Minority_Group_Cardinality
            # Split_Permute_Value is the overlap of minority states that we actually observe in the data.
            Split_Permute_Value = Reference_Gene_Minority_Group_Overlaps[RF_Inds]
            # Num_Divergent_Cell identifies how many samples in the RF/QF lead to observable divergence
            Num_Divergent_Cell = Minority_Group_Cardinality - Split_Permute_Value
            # Identify Split Direction (whether the observed arrangment is sorting towards the global minimum entropy or not.
            Sort_Into_Inds = np.where((Split_Permute_Value - Max_Entropy_Permutation) >= 0)[0]
            Split_Direction = 1
            Split_Permute_Entropies, Max_Permuation_Entropies, Minimum_Entropies = Calculate_Fixed_RF_Sort_Values(2,Split_Direction,Max_Entropy_Permutation[Sort_Into_Inds],Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables[RF_Inds[Sort_Into_Inds]],Min_Entropy_ID_1[Sort_Into_Inds],Min_Entropy_ID_2[Sort_Into_Inds],Split_Permute_Value[Sort_Into_Inds])
            ## Calculate divergence
            Divergences = Split_Permute_Entropies - Minimum_Entropies
            # Find the average divergence for each cell that is diverging from the optimal sort.
            Dependent_Divergence_Per_Cell = Divergences / Num_Divergent_Cell[Sort_Into_Inds]
            # Calculate how much divergence each cell would have if the RF/QF system was at the maximum entropy arrangment.
            Max_Num_Dependent_Divergence_Per_Cell = Min_Entropy_ID_2[Sort_Into_Inds] - Max_Entropy_Permutation[Sort_Into_Inds]
            Independence_Divergence_Per_Cell = (Max_Permuation_Entropies)/Max_Num_Dependent_Divergence_Per_Cell
            # Calculate the Error Potential according to ES hypothesis testing.
            Error_Potential = Dependent_Divergence_Per_Cell - Independence_Divergence_Per_Cell
            # Null/Ignore points that aren't usable.
            Error_Potential[np.isinf(Error_Potential)] = 0
            Error_Potential[np.isnan(Error_Potential)] = 0
            # Feature pairs with positive EPs provide evidence of erroneous datapoints.
            Positive_EP_Inds = np.where(Error_Potential > 0)[0]
            # Get Overlap Matrix
            Sort_Genes = Minority_Group_Matrix[np.ix_(Fixed_Gene_Minority_States,RF_Inds[Sort_Into_Inds][Positive_EP_Inds])]
            Sort_Into_Partitioning_Divergences = np.zeros(Cell_Cardinality)
            Sort_Into_Partitioning_Divergences[Fixed_Gene_Minority_States] = np.dot(Sort_Genes==0,Error_Potential[Positive_EP_Inds])                      
            ##### Perform caclulations when the FF gene is the QF and SD = -1 #####
            # Extract the group 1 and group 2 cardinalities. Group 1 is always the minority group in this set up.
            Minority_Group_Cardinality = Permutables[QF_Inds]
            Majority_Group_Cardinality = Cell_Cardinality - Permutables[QF_Inds]
            Permutable = Permutables[Feature_Inds]
            # Maximum entropy of the system is identified from the derivative of the Entropy Sorting Equation (ESQ)
            Max_Entropy_Permutation = (Minority_Group_Cardinality * Permutable)/(Minority_Group_Cardinality + Majority_Group_Cardinality)
            # The maximum and minimum points of the ESE are identified from the boundaries of the ESE curve.
            Min_Entropy_ID_1 = np.zeros(QF_Inds.shape[0])
            Min_Entropy_ID_2 = np.repeat(Permutable,QF_Inds.shape[0])
            Check_Fits_Group_1 = Minority_Group_Cardinality - Min_Entropy_ID_2
            # If the minority group of the QF is larger than the minority group of the RF then the boundary point is the cardinality of the RF minority group.
            Min_Entropy_ID_2[np.where(Check_Fits_Group_1 < 0)[0]] = Minority_Group_Cardinality[np.where(Check_Fits_Group_1 < 0)[0]]
            # Split_Permute_Value is the overlap of minority states that we actually observe in the data.
            Split_Permute_Value = Reference_Gene_Minority_Group_Overlaps[QF_Inds]
            # Num_Divergent_Cell identifies how many samples in the RF/QF lead to observable divergence
            Num_Divergent_Cell = Minority_Group_Cardinality - Split_Permute_Value            
            # Identify Split Direction (whether the observed arrangment is sorting towards the global minimum entropy or not.
            Sort_Out_Of_Inds = np.where((Split_Permute_Value - Max_Entropy_Permutation) < 0)[0]
            Split_Direction = -1
            Split_Permute_Entropies, Max_Permuation_Entropies, Minimum_Entropies = Calculate_Fixed_QF_Sort_Values(2,Split_Direction,Permutable,Max_Entropy_Permutation[Sort_Out_Of_Inds],Minority_Group_Cardinality[Sort_Out_Of_Inds],Majority_Group_Cardinality[Sort_Out_Of_Inds],Min_Entropy_ID_1[Sort_Out_Of_Inds],Min_Entropy_ID_2[Sort_Out_Of_Inds],Split_Permute_Value[Sort_Out_Of_Inds])
            ## Calculate divergence
            Divergences = Split_Permute_Entropies - Minimum_Entropies
            # Find the average divergence for each cell that is diverging from the optimal sort.
            Dependent_Divergence_Per_Cell = Divergences / Split_Permute_Value[Sort_Out_Of_Inds]
            # Calculate how much divergence each cell would have if the RF/QF system was at the maximum entropy arrangment.
            Max_Num_Dependent_Divergence_Per_Cell = Max_Entropy_Permutation[Sort_Out_Of_Inds]
            Independence_Divergence_Per_Cell = (Max_Permuation_Entropies)/Max_Num_Dependent_Divergence_Per_Cell
            # Calculate the Error Potential according to ES hypothesis testing.
            Error_Potential = Dependent_Divergence_Per_Cell - Independence_Divergence_Per_Cell
            # Null/Ignore points that aren't usable.
            Error_Potential[np.isinf(Error_Potential)] = 0
            Error_Potential[np.isnan(Error_Potential)] = 0
            # Feature pairs with positive EPs provide evidence of erroneous datapoints.
            Positive_EP_Inds = np.where(Error_Potential > 0)[0]
            # Get Overlap Matrix
            Sort_Genes = Minority_Group_Matrix[np.ix_(Fixed_Gene_Minority_States,QF_Inds[Sort_Out_Of_Inds][Positive_EP_Inds])]
            Sort_Out_Of_Permuting_Divergences = np.zeros(Cell_Cardinality)
            Sort_Out_Of_Permuting_Divergences[Fixed_Gene_Minority_States] = np.dot(Sort_Genes,Error_Potential[Positive_EP_Inds])
            return Sort_Out_Of_Partitioning_Divergences + Sort_Into_Partitioning_Divergences + Sort_Out_Of_Permuting_Divergences
        if Error_Type == "2": # Caluclate divergences for scenarios where divergence indicates false negative data points.
            ##### Get Inspected Gene Information #####
            # Extract the group 1 and group 2 cardinalities. Group 1 is always the minority group in this set up.
            Minority_Group_Cardinality = Permutables[Feature_Inds]
            Majority_Group_Cardinality = Cell_Cardinality - Minority_Group_Cardinality
            # Identify in which scenarios the inspected gene is the Partitioning Gene and which it is the Permutable Gene
            # Whenever the inspected gene minority state cardinality is smaller than any other gene's, it is the RF
            RF_Inds = np.where(Permutables >= Minority_Group_Cardinality)[0]
            # Whenever the inspected gene minority state cardinality is larger than any other gene's, it is the QF
            QF_Inds = np.where(Permutables < Minority_Group_Cardinality)[0]
            ##### Perform caclulations when the FF gene is the QF and SD = 1 #####
            # Extract the group 1 and group 2 cardinalities. Group 1 is always the minority group in this set up.
            Minority_Group_Cardinality = Permutables[QF_Inds]
            Majority_Group_Cardinality = Cell_Cardinality - Permutables[QF_Inds]
            Permutable = Permutables[Feature_Inds]
            # Maximum entropy of the system is identified from the derivative of the Entropy Sorting Equation (ESE)
            Max_Entropy_Permutation = (Minority_Group_Cardinality * Permutable)/(Minority_Group_Cardinality + Majority_Group_Cardinality)
            # The maximum and minimum points of the ESE are identified from the boundaries of the ESE curve.
            Min_Entropy_ID_1 = np.zeros(QF_Inds.shape[0])
            Min_Entropy_ID_2 = np.repeat(Permutable,QF_Inds.shape[0])
            Check_Fits_Group_1 = Minority_Group_Cardinality - Min_Entropy_ID_2
            # If the minority group of the QF is larger than the minority group of the RF then the boundary point is the cardinality of the RF minority group.
            Min_Entropy_ID_2[np.where(Check_Fits_Group_1 < 0)[0]] = Minority_Group_Cardinality[np.where(Check_Fits_Group_1 < 0)[0]]
            # Split_Permute_Value is the overlap of minority states that we actually observe in the data.
            Split_Permute_Value = Reference_Gene_Minority_Group_Overlaps[QF_Inds]
            # Num_Divergent_Cell
            Num_Divergent_Cell = Minority_Group_Cardinality - Split_Permute_Value
            # Identify Split Direction (whether the observed arrangment is sorting towards the global minimum entropy or not.
            Sort_Into_Inds = np.where((Split_Permute_Value - Max_Entropy_Permutation) >= 0)[0]
            Split_Direction = 1
            Split_Permute_Entropies, Max_Permuation_Entropies, Minimum_Entropies = Calculate_Fixed_QF_Sort_Values(2,Split_Direction,Permutable,Max_Entropy_Permutation[Sort_Into_Inds],Minority_Group_Cardinality[Sort_Into_Inds],Majority_Group_Cardinality[Sort_Into_Inds],Min_Entropy_ID_1[Sort_Into_Inds],Min_Entropy_ID_2[Sort_Into_Inds],Split_Permute_Value[Sort_Into_Inds])
            ## Calculate divergence 
            Divergences = Split_Permute_Entropies - Minimum_Entropies
            # Find the average divergence for each cell that is diverging from the optimal sort.
            Dependent_Divergence_Per_Cell = Divergences / Num_Divergent_Cell[Sort_Into_Inds]
            # Calculate how much divergence each cell would have if the RF/QF system was at the maximum entropy arrangment.
            Max_Num_Dependent_Divergence_Per_Cell = Min_Entropy_ID_2[Sort_Into_Inds] - Max_Entropy_Permutation[Sort_Into_Inds]
            Independence_Divergence_Per_Cell = (Max_Permuation_Entropies)/Max_Num_Dependent_Divergence_Per_Cell
            # Calculate the Error Potential according to ES hypothesis testing.
            Error_Potential = Dependent_Divergence_Per_Cell - Independence_Divergence_Per_Cell
            # Null/Ignore points that aren't usable.
            Error_Potential[np.isinf(Error_Potential)] = 0
            Error_Potential[np.isnan(Error_Potential)] = 0
            # Feature pairs with positive EPs provide evidence of erroneous datapoints.
            Positive_EP_Inds = np.where(Error_Potential > 0)[0]
            # Get Overlap Matrix
            Sort_Genes = Minority_Group_Matrix[np.ix_(Fixed_Gene_Majority_States,QF_Inds[Sort_Into_Inds][Positive_EP_Inds])]
            Sort_Into_Permuting_Divergences = np.zeros(Cell_Cardinality)
            Sort_Into_Permuting_Divergences[Fixed_Gene_Majority_States] = np.dot(Sort_Genes,Error_Potential[Positive_EP_Inds])
            return Sort_Into_Permuting_Divergences
        if Error_Type == "2_Intentional_Error": # Caluclate divergences for scenarios where divergence indicates false negative data points,
            # but this time, the output is the observed divergences for each RF/QF pair, for use in ESFW.
            ##### Get Inspected Gene Information #####
            # Extract the group 1 and group 2 cardinalities. Group 1 is always the minority group in this set up.
            Minority_Group_Cardinality = Permutables[Feature_Inds]
            Majority_Group_Cardinality = Cell_Cardinality - Minority_Group_Cardinality
            # Identify in which scenarios the inspected gene is the Partitioning Gene and which it is the Permutable Gene
            # Whenever the inspected gene minority state cardinality is smaller than any other gene's, it is the RF
            RF_Inds = np.where(Permutables >= Minority_Group_Cardinality)[0]
            # Whenever the inspected gene minority state cardinality is larger than any other gene's, it is the QF
            QF_Inds = np.where(Permutables < Minority_Group_Cardinality)[0]
            ##### Perform caclulations when the FF gene is the QF and SD = 1 #####
            # Extract the group 1 and group 2 cardinalities. Group 1 is always the minority group in this set up.
            Minority_Group_Cardinality = Permutables[QF_Inds]
            Majority_Group_Cardinality = Cell_Cardinality - Permutables[QF_Inds]
            Permutable = Permutables[Feature_Inds]
            # Maximum entropy of the system is identified from the derivative of the Entropy Sorting Equation (ESE)
            Max_Entropy_Permutation = (Minority_Group_Cardinality * Permutable)/(Minority_Group_Cardinality + Majority_Group_Cardinality)
            # The maximum and minimum points of the ESE are identified from the boundaries of the ESE curve.
            Min_Entropy_ID_1 = np.zeros(QF_Inds.shape[0])
            Min_Entropy_ID_2 = np.repeat(Permutable,QF_Inds.shape[0])
            Check_Fits_Group_1 = Minority_Group_Cardinality - Min_Entropy_ID_2
            # If the minority group of the QF is larger than the minority group of the RF then the boundary point is the cardinality of the RF minority group.
            Min_Entropy_ID_2[np.where(Check_Fits_Group_1 < 0)[0]] = Minority_Group_Cardinality[np.where(Check_Fits_Group_1 < 0)[0]]
            # Split_Permute_Value is the overlap of minority states that we actually observe in the data.
            Split_Permute_Value = Reference_Gene_Minority_Group_Overlaps[QF_Inds]
            # Num_Divergent_Cell
            Num_Divergent_Cell = Minority_Group_Cardinality - Split_Permute_Value
            # Identify Split Direction (whether the observed arrangment is sorting towards the global minimum entropy or not.
            Sort_Into_Inds = np.where((Split_Permute_Value - Max_Entropy_Permutation) >= 0)[0]
            Split_Direction = 1
            Split_Permute_Entropies, Max_Permuation_Entropies, Minimum_Entropies = Calculate_Fixed_QF_Sort_Values(2,Split_Direction,Permutable,Max_Entropy_Permutation[Sort_Into_Inds],Minority_Group_Cardinality[Sort_Into_Inds],Majority_Group_Cardinality[Sort_Into_Inds],Min_Entropy_ID_1[Sort_Into_Inds],Min_Entropy_ID_2[Sort_Into_Inds],Split_Permute_Value[Sort_Into_Inds])
            ## Calculate divergence 
            Divergences = Split_Permute_Entropies - Minimum_Entropies
            # Find the average divergence for each cell that is diverging from the optimal sort.
            Dependent_Divergence_Per_Cell = Divergences / Num_Divergent_Cell[Sort_Into_Inds]
            # Calculate how much divergence each cell would have if the RF/QF system was at the maximum entropy arrangment.
            Max_Num_Dependent_Divergence_Per_Cell = Min_Entropy_ID_2[Sort_Into_Inds] - Max_Entropy_Permutation[Sort_Into_Inds]
            Independence_Divergence_Per_Cell = (Max_Permuation_Entropies)/Max_Num_Dependent_Divergence_Per_Cell
            # Calculate the Error Potential according to ES hypothesis testing.
            Error_Potential = Dependent_Divergence_Per_Cell - Independence_Divergence_Per_Cell
            # Null/Ignore points that aren't usable.
            Error_Potential[np.isinf(Error_Potential)] = 0
            Error_Potential[np.isnan(Error_Potential)] = 0
            # Feature pairs with positive EPs provide evidence of erroneous datapoints.
            Positive_EP_Inds = np.where(Error_Potential > 0)[0]
            # Get average feature divergences (which correspond to feature weights in ESFW)
            if Positive_EP_Inds.shape[0] > 0:
                Feature_Divergence = np.mean(Error_Potential[Positive_EP_Inds])
            else:
                Feature_Divergence = 0
            return Feature_Divergence
    else:
        # When a feature cannot be used just give all points a value of 0.
        if Error_Type != "2_Intentional_Error":
            return np.zeros(Cell_Cardinality)
        else:
            return 0


def Calculate_Fixed_QF_Sort_Values(Outputs,Split_Direction,Permutable,Max_Entropy_Permutation,Minority_Group_Cardinality,Majority_Group_Cardinality,Min_Entropy_ID_1,Min_Entropy_ID_2,Split_Permute_Value):
    # Calculate critical points on the ES curve
    Max_Permuation_Entropies = Calc_QF_Entropies(Max_Entropy_Permutation,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutable)
    if Split_Direction == -1 and Outputs != 1:
        # The minimum entropy if none of the QF minority states are in the RF minority group.
        Minimum_Entropies = Calc_QF_Entropies(Min_Entropy_ID_1,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutable)
    if Split_Direction == 1 and Outputs != 1:
        # The minimum entropy if the RF minority group has as many of the QF minority state samples in it as possible.
        Minimum_Entropies = Calc_QF_Entropies(Min_Entropy_ID_2,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutable)
    # The entropy of the arrangment observed in the data set.
    Split_Permute_Entropies = Calc_QF_Entropies(Split_Permute_Value,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutable)
    if Outputs == 1:
        return Split_Permute_Entropies, Max_Permuation_Entropies
    if Outputs == 2:
        return Split_Permute_Entropies, Max_Permuation_Entropies, Minimum_Entropies
    if Outputs == 3:
        # Calculate ES parabola properties
        Max_Entropy_Differences = Max_Permuation_Entropies - Minimum_Entropies
        Entropy_Losses = Max_Permuation_Entropies - Split_Permute_Entropies
        # Vector of Information Gain values for each QF/RF pair.
        Sort_Gains = Entropy_Losses/Max_Entropy_Differences
        # Vector of Split Weights values for each QF/RF pair.
        Sort_Weights = (Max_Permuation_Entropies - Minimum_Entropies) / Max_Permuation_Entropies
        return Sort_Gains, Sort_Weights


### Caclcuate entropies based on group cardinalities with fixed QF
def Calc_QF_Entropies(x,Group1_Cardinality,Group2_Cardinality,Permutable):
    ## Entropy Sort Equation (ESQ) is split into for parts for convenience
    # Equation 1
    Eq_1 = np.zeros(x.shape[0])
    Calculate_Inds = np.where((x/Group1_Cardinality) > 0)[0]
    Eq_1[Calculate_Inds] = - (x[Calculate_Inds]/Group1_Cardinality[Calculate_Inds])*np.log2(x[Calculate_Inds]/Group1_Cardinality[Calculate_Inds])  
    # Equation 2  
    Eq_2 = np.zeros(x.shape[0])
    Calculate_Inds = np.where(((Group1_Cardinality - x)/Group1_Cardinality) > 0)[0]
    Eq_2[Calculate_Inds] = - ((Group1_Cardinality[Calculate_Inds] - x[Calculate_Inds])/Group1_Cardinality[Calculate_Inds])*np.log2(((Group1_Cardinality[Calculate_Inds] - x[Calculate_Inds])/Group1_Cardinality[Calculate_Inds]))  
    # Equation 3  
    Eq_3 = np.zeros(x.shape[0])
    Calculate_Inds = np.where(((Permutable - x) / Group2_Cardinality) > 0)[0]
    Eq_3[Calculate_Inds] = - ((Permutable - x[Calculate_Inds]) / Group2_Cardinality[Calculate_Inds])*np.log2(((Permutable - x[Calculate_Inds]) / Group2_Cardinality[Calculate_Inds]))
    # Equation 4  
    Eq_4 = np.zeros(x.shape[0])
    Calculate_Inds = np.where(((Group2_Cardinality-Permutable+x)/Group2_Cardinality) > 0)[0]
    Eq_4[Calculate_Inds] = - ((Group2_Cardinality[Calculate_Inds]-Permutable+x[Calculate_Inds])/Group2_Cardinality[Calculate_Inds])*np.log2(((Group2_Cardinality[Calculate_Inds]-Permutable+x[Calculate_Inds])/Group2_Cardinality[Calculate_Inds]))
    # Calculate overall entropy for each QF/RF pair
    Entropy = (Group1_Cardinality/(Group1_Cardinality+Group2_Cardinality))*(Eq_1 + Eq_2) + (Group2_Cardinality/(Group1_Cardinality+Group2_Cardinality))*(Eq_3 + Eq_4)
    return Entropy


def Calculate_Fixed_RF_Sort_Values(Outputs,Split_Direction,Max_Entropy_Permutation,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables,Min_Entropy_ID_1,Min_Entropy_ID_2,Split_Permute_Value):
    # Calculate critical points on the ES curve
    Max_Permuation_Entropies = Calc_RF_Entropies(Max_Entropy_Permutation,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables)
    if Split_Direction == -1 and Outputs != 1:
        # The minimum entropy if none of the QF minority states are in the RF minority group.
        Minimum_Entropies = Calc_RF_Entropies(Min_Entropy_ID_1,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables)
    if Split_Direction == 1 and Outputs != 1:
        # The minimum entropy if the RF minority group has as many of the QF minority state samples in it as possible.
        Minimum_Entropies = Calc_RF_Entropies(Min_Entropy_ID_2,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables)
    # The entropy of the arrangment observed in the data set.
    Split_Permute_Entropies = Calc_RF_Entropies(Split_Permute_Value,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables)
    if Outputs == 1:
        return Split_Permute_Entropies, Max_Permuation_Entropies
    if Outputs == 2:
        return Split_Permute_Entropies, Max_Permuation_Entropies, Minimum_Entropies
    if Outputs == 3:
        # Calculate ES parabola properties
        Max_Entropy_Differences = Max_Permuation_Entropies - Minimum_Entropies
        Entropy_Losses = Max_Permuation_Entropies - Split_Permute_Entropies
        # Vector of Information Gain values for each RF/QF pair.
        Sort_Gains = Entropy_Losses/Max_Entropy_Differences
        # Vector of Split Weights values for each RF/QF pair.
        Sort_Weights = (Max_Permuation_Entropies - Minimum_Entropies) / Max_Permuation_Entropies
        return Sort_Gains, Sort_Weights


### Caclcuate entropies based on group cardinalities with fixed RF
def Calc_RF_Entropies(x,Group1_Cardinality,Group2_Cardinality,Permutables):
    ## Entropy Sort Equation (ESQ) is split into for parts for convenience
    # Equation 1
    Eq_1 = np.zeros(x.shape[0])
    Calculate_Inds = np.where((x/Group1_Cardinality) > 0)[0]
    Eq_1[Calculate_Inds] = - (x[Calculate_Inds]/Group1_Cardinality)*np.log2(x[Calculate_Inds]/Group1_Cardinality)  
    # Equation 2  
    Eq_2 = np.zeros(x.shape[0])
    Calculate_Inds = np.where(((Group1_Cardinality - x)/Group1_Cardinality) > 0)[0]
    Eq_2[Calculate_Inds] = - ((Group1_Cardinality - x[Calculate_Inds])/Group1_Cardinality)*np.log2(((Group1_Cardinality - x[Calculate_Inds])/Group1_Cardinality))  
    # Equation 3  
    Eq_3 = np.zeros(x.shape[0])
    Calculate_Inds = np.where(((Permutables - x) / Group2_Cardinality) > 0)[0]
    Eq_3[Calculate_Inds] = - ((Permutables[Calculate_Inds] - x[Calculate_Inds]) / Group2_Cardinality)*np.log2(((Permutables[Calculate_Inds] - x[Calculate_Inds]) / Group2_Cardinality))
    # Equation 4  
    Eq_4 = np.zeros(x.shape[0])
    Calculate_Inds = np.where(((Group2_Cardinality-Permutables+x)/Group2_Cardinality) > 0)[0]
    Eq_4[Calculate_Inds] = - ((Group2_Cardinality-Permutables[Calculate_Inds]+x[Calculate_Inds])/Group2_Cardinality)*np.log2(((Group2_Cardinality-Permutables[Calculate_Inds]+x[Calculate_Inds])/Group2_Cardinality))
    # Calculate overall entropy for each RF/QF pair
    Entropy = (Group1_Cardinality/(Group1_Cardinality+Group2_Cardinality))*(Eq_1 + Eq_2) + (Group2_Cardinality/(Group1_Cardinality+Group2_Cardinality))*(Eq_3 + Eq_4)
    return Entropy


### Here we have the ESFW wrapper function that executes all the steps of ESFW. ###
## Dropout_Fraction: The fixed proportion of drop outs to be intentionally added to the minority states of each feature in the discretised
# matrix. Can take any value between 0 and 1.
## Binarised_Input_Matrix: The discretised matrix where rows are samples and columns are features. This must be the same discretisaed matrix that was input into FFAVES.
# Each sample for each feature should be represented by either a 0 or a 1. It does not matter if your 0's or 1's represent the minority state for each feature,
# ESFW will work it out.
## Track_Imputation_Steps: The suggested imputations after each cycle of FFAVES is applied to the Binarised_Input_Matrix. This is output by FFAVES.
## Chosen_Cycle: The chosen cycle of FFAVES suggested imputations that will be used to corrected suprious false positive and false negative data points
# before undergoing feature weighting via ESFW. A good starting point is to use the default value of -1 as this is the point where FFAVES has converged
# on a set of FP/FN data points. However, in our pre-implantation human embryo example from out paper, we present a rational for selecting a more optimal
# value for Chosen_Cycle. See the code within the provided file called "Meistermann_Feature_Selection.py" for details.
## Allows us to subset to a partiuclar set of features using a numpy array of numerical indicies that we wish to use.
# Default of "All" uses all features (columns) in the data.
## Itterations: Number of drop out simulations to caclulate feature divergences on. The final weights will be the average of these itterations.
## Min_Clust_Size: The minimum number of samples a feature can have in the minority state before it is ignored.
## Use_Cores: The number of cores used. When equal to -1, will use one less than the number of cores on your machine.
## Auto_Save: If 1, objected generated by FFAVES will be saved after each cycle. If any other number, object will not be automatically saved.

def ESFW(Dropout_Fraction, Binarised_Input_Matrix, Track_Imputation_Steps, Chosen_Cycle = -1, Use_Features_Ind = np.array(["All"]), Itterations = 5, Min_Clust_Size = 5, Use_Cores = -1, Auto_Save = 1):
    # Set number of cores to use
    Cores_Available = multiprocess.cpu_count()
    if Use_Cores == -1:
        Use_Cores = Cores_Available - 1 # -1 Is an arbitrary buffer of idle cores that I set.
        if Use_Cores < 1:
            Use_Cores = 1
    print("Cores Avaiblable: " + str(Cores_Available))
    print("Cores Used: " + str(Use_Cores))
    # Define data dimensions
    #global Cell_Cardinality
    Cell_Cardinality = Binarised_Input_Matrix.shape[0]
    #global Gene_Cardinality
    Gene_Cardinality = Binarised_Input_Matrix.shape[1]
    # Convert Binarised_Input_Matrix into Minority_Group_Matrix
    Permutables, Switch_State_Inidicies = Find_Permutations(Binarised_Input_Matrix,Cell_Cardinality)
    Binarised_Input_Matrix[:,Switch_State_Inidicies] = (Binarised_Input_Matrix[:,Switch_State_Inidicies] * -1) + 1  
    # Set up Minority_Group_Matrix
    global Minority_Group_Matrix
    print("Number of cells: " + str(Cell_Cardinality))
    print("Number of genes: " + str(Gene_Cardinality))
    Minority_Group_Matrix = copy.copy(Binarised_Input_Matrix)
    # Convert suggested imputation points to correct state for chosen cycle.
    if Chosen_Cycle >= 0:
        Suggested_Impute_Inds = Track_Imputation_Steps[Chosen_Cycle][2]
        Minority_Group_Matrix[Suggested_Impute_Inds] = (Minority_Group_Matrix[Suggested_Impute_Inds] - 1) * -1 
        Step_1_Type_1_Error_Inds = Track_Imputation_Steps[Chosen_Cycle+1][0]
        Minority_Group_Matrix[Step_1_Type_1_Error_Inds] = (Minority_Group_Matrix[Step_1_Type_1_Error_Inds] - 1) * -1
    if Chosen_Cycle < 0:
        Suggested_Impute_Inds = Track_Imputation_Steps[Chosen_Cycle-1][2]
        Minority_Group_Matrix[Suggested_Impute_Inds] = (Minority_Group_Matrix[Suggested_Impute_Inds] - 1) * -1 
        Step_1_Type_1_Error_Inds = Track_Imputation_Steps[Chosen_Cycle][0]
        Minority_Group_Matrix[Step_1_Type_1_Error_Inds] = (Minority_Group_Matrix[Step_1_Type_1_Error_Inds] - 1) * -1
    if Use_Features_Ind[0] != "All":
       Minority_Group_Matrix = Minority_Group_Matrix[:,Use_Features_Ind] 
    Cycle_Suggested_Imputations = np.where(Binarised_Input_Matrix != Minority_Group_Matrix)
    Feature_Divergences = []
    for Cycle in np.arange(Itterations):
        print("Cycle Number: " + str(Cycle+1) + " of " + str(Itterations))
        Minority_Group_Matrix = copy.copy(Binarised_Input_Matrix)
        Minority_Group_Matrix[Cycle_Suggested_Imputations] = (Minority_Group_Matrix[Cycle_Suggested_Imputations] * -1) + 1
        # Remove genes below Min_Clust_Size
        Keep_Features = np.where(np.sum(Minority_Group_Matrix,axis=0) >= Min_Clust_Size)[0]
        Ignore = np.where(np.sum(Minority_Group_Matrix,axis=0) >= (Minority_Group_Matrix.shape[1]-Min_Clust_Size))[0] 
        Keep_Features = np.delete(Keep_Features,np.where(np.isin(Keep_Features,Ignore))[0])
        if Keep_Features.shape[0] < Minority_Group_Matrix.shape[1]:
            print("Ignoring " + str(Minority_Group_Matrix.shape[1]-Keep_Features.shape[0]) + " features which are below the Min_Clust_Threshold")
            Minority_Group_Matrix = Minority_Group_Matrix[:,Keep_Features]
        #global Cell_Cardinality
        Cell_Cardinality = Minority_Group_Matrix.shape[0]
        #global Gene_Cardinality
        Gene_Cardinality = Minority_Group_Matrix.shape[1]
        Permutables, Switch_State_Inidicies = Find_Permutations(Minority_Group_Matrix,Cell_Cardinality)
        # Switch Minority/Majority states to 0/1 where necessary. 
        Minority_Group_Matrix[:,Switch_State_Inidicies] = (Minority_Group_Matrix[:,Switch_State_Inidicies] * -1) + 1
        # Intentionally Add Error
        for i in np.arange(Minority_Group_Matrix.shape[1]):
            Feature_Minority_Inds = np.where(Minority_Group_Matrix[:,i] == 1)[0]
            Null = np.random.choice(Feature_Minority_Inds.shape[0], int(Feature_Minority_Inds.shape[0]*Dropout_Fraction),replace=False)
            Minority_Group_Matrix[Feature_Minority_Inds[Null],i] = 0
        Permutables, Switch_State_Inidicies = Find_Permutations(Minority_Group_Matrix,Cell_Cardinality)
        # Switch Minority/Majority states to 0/1 where necessary. 
        Minority_Group_Matrix[:,Switch_State_Inidicies] = (Minority_Group_Matrix[:,Switch_State_Inidicies] * -1) + 1
        # Calculate minority group overlap matrix
        Reference_Gene_Minority_Group_Overlaps = Parallel_Find_Minority_Group_Overlaps(Use_Cores,Gene_Cardinality)
        Permutables[Permutables < Min_Clust_Size] = np.nan
        with np.errstate(divide='ignore',invalid='ignore'):
            Feature_Divergence = Parallel_Calculate_Dependent_Divergence_Per_Cell("2_Intentional_Error",Cell_Cardinality,Gene_Cardinality,Permutables,Reference_Gene_Minority_Group_Overlaps,Use_Cores)
        Feature_Divergences.append(Feature_Divergence)
    Feature_Divergences = np.asarray(Feature_Divergences)
    if Auto_Save == 1:
        np.save("Feature_Divergences.npy",Feature_Divergences)
        np.save("Feature_Divergences_Used_Inds.npy",Keep_Features)
    return Feature_Divergences, Cycle_Suggested_Imputations, Keep_Features


#### Here we have additional functions to help with analysis of the results ####

## Retreive the Sort Gains Matrix and Sort Weights Matrix for a given cycle ##

def Calculate_ES_Sort_Matricies(Binarised_Input_Matrix, Track_Imputation_Steps, Chosen_Cycle = -1, Use_Features_Ind = np.array(["All"]), Min_Clust_Size = 5, Use_Cores = -1, Auto_Save = 1, Observe_Directionality = 0):
    # Set number of cores to use
    Cores_Available = multiprocess.cpu_count()
    if Use_Cores == -1:
        Use_Cores = Cores_Available - 1 # -1 Is an arbitrary buffer of idle cores that I set.
        if Use_Cores < 1:
            Use_Cores = 1
    print("Cores Avaiblable: " + str(Cores_Available))
    print("Cores Used: " + str(Use_Cores))
    # Define data dimensions
    #global Cell_Cardinality
    Cell_Cardinality = Binarised_Input_Matrix.shape[0]
    #global Gene_Cardinality
    Gene_Cardinality = Binarised_Input_Matrix.shape[1]
    # Convert Binarised_Input_Matrix into Minority_Group_Matrix
    Permutables, Switch_State_Inidicies = Find_Permutations(Binarised_Input_Matrix,Cell_Cardinality)
    Binarised_Input_Matrix[:,Switch_State_Inidicies] = (Binarised_Input_Matrix[:,Switch_State_Inidicies] * -1) + 1  
    # Set up Minority_Group_Matrix
    global Minority_Group_Matrix
    Minority_Group_Matrix = copy.copy(Binarised_Input_Matrix)
    # Convert suggested imputation points to correct state for chosen cycle.
    if Chosen_Cycle >= 0:
        Suggested_Impute_Inds = Track_Imputation_Steps[Chosen_Cycle][2]
        Minority_Group_Matrix[Suggested_Impute_Inds] = (Minority_Group_Matrix[Suggested_Impute_Inds] - 1) * -1 
        Step_1_Type_1_Error_Inds = Track_Imputation_Steps[Chosen_Cycle+1][0]
        Minority_Group_Matrix[Step_1_Type_1_Error_Inds] = (Minority_Group_Matrix[Step_1_Type_1_Error_Inds] - 1) * -1
    if Chosen_Cycle < 0:
        Suggested_Impute_Inds = Track_Imputation_Steps[Chosen_Cycle-1][2]
        Minority_Group_Matrix[Suggested_Impute_Inds] = (Minority_Group_Matrix[Suggested_Impute_Inds] - 1) * -1 
        Step_1_Type_1_Error_Inds = Track_Imputation_Steps[Chosen_Cycle][0]
        Minority_Group_Matrix[Step_1_Type_1_Error_Inds] = (Minority_Group_Matrix[Step_1_Type_1_Error_Inds] - 1) * -1
    Cycle_Suggested_Imputations = np.where(Binarised_Input_Matrix != Minority_Group_Matrix)
    if Use_Features_Ind[0] != "All":
       Minority_Group_Matrix = Minority_Group_Matrix[:,Use_Features_Ind] 
    # Remove genes below Min_Clust_Size
    Keep_Features = np.where(np.sum(Minority_Group_Matrix,axis=0) >= Min_Clust_Size)[0]
    Ignore = np.where(np.sum(Minority_Group_Matrix,axis=0) >= (Minority_Group_Matrix.shape[1]-Min_Clust_Size))[0] 
    Keep_Features = np.delete(Keep_Features,np.where(np.isin(Keep_Features,Ignore))[0])
    if Keep_Features.shape[0] < Minority_Group_Matrix.shape[1]:
        print("Ignoring " + str(Minority_Group_Matrix.shape[1]-Keep_Features.shape[0]) + " features which are below the Min_Clust_Threshold")
        Minority_Group_Matrix = Minority_Group_Matrix[:,Keep_Features]
    #Cell_Cardinality
    Cell_Cardinality = Minority_Group_Matrix.shape[0]
    #Gene_Cardinality
    Gene_Cardinality = Minority_Group_Matrix.shape[1]   
    print("Number of cells: " + str(Cell_Cardinality))
    print("Number of genes: " + str(Gene_Cardinality))
    if Gene_Cardinality >= 30:    
        Permutables, Switch_State_Inidicies = Find_Permutations(Minority_Group_Matrix,Cell_Cardinality)
        # Switch Minority/Majority states to 0/1 where necessary. 
        Minority_Group_Matrix[:,Switch_State_Inidicies] = (Minority_Group_Matrix[:,Switch_State_Inidicies] * -1) + 1
        # Calculate minority group overlap matrix
        Reference_Gene_Minority_Group_Overlaps = Parallel_Find_Minority_Group_Overlaps(Use_Cores,Gene_Cardinality)
        Permutables[Permutables < Min_Clust_Size] = np.nan
        print("Performing Sort Calculations")
        with np.errstate(divide='ignore',invalid='ignore'):
            Sort_Gains, Sort_Weights = Parallel_Calculate_ES_Matricies(Cell_Cardinality,Gene_Cardinality,Permutables,Reference_Gene_Minority_Group_Overlaps,Use_Cores,Observe_Directionality)
        if Observe_Directionality == 0:
            Double_Counts = np.where(np.logical_and(Sort_Gains != 0, Sort_Gains.T != 0))
            Sort_Gains = Sort_Gains + Sort_Gains.T
            Sort_Gains[Double_Counts] = Sort_Gains[Double_Counts] / 2
            Sort_Weights = Sort_Weights + Sort_Weights.T
            Sort_Weights[Double_Counts] = Sort_Weights[Double_Counts] / 2
        if Auto_Save == 1:
            np.save("Sort_Gains.npy",Sort_Gains)
            np.save("Sort_Weights.npy",Sort_Weights)
            np.save("Cycle_Suggested_Imputations.npy",Cycle_Suggested_Imputations)
            np.save("ES_Matrices_Features_Used_Inds.npy",Keep_Features)
        return Sort_Gains, Sort_Weights, Cycle_Suggested_Imputations, Keep_Features
    else:
        print("Less than 30 features pass the minimum group size threshold. No calculations performed.")
        return "No Results", "No Results", "No Results", "No Results"


def Parallel_Calculate_ES_Matricies(Cell_Cardinality,Gene_Cardinality,Permutables,Reference_Gene_Minority_Group_Overlaps,Use_Cores,Observe_Directionality):
    Feature_Inds = np.arange(Gene_Cardinality)
    Pass_Info_To_Cores = np.concatenate((Feature_Inds.reshape(1,Feature_Inds.shape[0]),Reference_Gene_Minority_Group_Overlaps))
    Pass_Info_To_Cores = np.transpose(Pass_Info_To_Cores)
    # Parrallel calculate information gains matrix
    pool = multiprocess.Pool(processes = Use_Cores)
    Results = pool.map(partial(Calculate_ES_Matricies,Cell_Cardinality=Cell_Cardinality,Permutables=Permutables,Observe_Directionality=Observe_Directionality), Pass_Info_To_Cores)
    pool.close()
    pool.join()
    Results = np.asarray(Results)
    # Retreive Information_Gain_Matrix
    # Retreive Sort_Gains and put the features back in the original feature ordering.
    Sort_Gains = Results[:,0]
    Sort_Gains[np.isnan(Sort_Gains)] = 0
    Sort_Gains[np.isinf(Sort_Gains)] = 0
    # Retreive Positive_EP_Inds and put the features back in the original feature ordering.
    Sort_Weights = Results[:,1]
    Sort_Weights[np.isnan(Sort_Weights)] = 0
    Sort_Weights[np.isinf(Sort_Weights)] = 0
    return Sort_Gains, Sort_Weights


def Calculate_ES_Matricies(Pass_Info_To_Cores,Cell_Cardinality,Permutables,Observe_Directionality):
    with np.errstate(divide='ignore',invalid='ignore'):
        # Extract which gene calculations are centred around
        Feature_Inds = int(Pass_Info_To_Cores[0])
        # Remove the Query Gene ind from the data vector
        Reference_Gene_Minority_Group_Overlaps = np.delete(Pass_Info_To_Cores,0)
        Results = []
        if np.isnan(Permutables[Feature_Inds]) == 0:
            ##### Fixed RF Caclulations #####
            # Extract the group 1 and group 2 cardinalities. Group 1 is always the minority group in this set up.
            Minority_Group_Cardinality = Permutables[Feature_Inds]
            Majority_Group_Cardinality = Cell_Cardinality - Minority_Group_Cardinality
            #Permutable = Permutables
            # Maximum entropy of the system is identified from the derivative of the Entropy Sorting Equation (ESQ)
            Max_Entropy_Permutation = (Minority_Group_Cardinality * Permutables)/(Minority_Group_Cardinality + Majority_Group_Cardinality)
            # The maximum and minimum points of the ESQ are identified from the boundaries of the ESQ curve.
            Min_Entropy_ID_1 = np.zeros(Permutables.shape[0])
            Min_Entropy_ID_2 = copy.copy(Permutables)
            Check_Fits_Group_1 = Minority_Group_Cardinality - Min_Entropy_ID_2
            # If the minority group of the QF is larger than the minority group of the RF then the boundary point is the cardinality of the RF minority group.
            Min_Entropy_ID_2[np.where(Check_Fits_Group_1 < 0)[0]] = Minority_Group_Cardinality
            # Split_Permute_Value is the overlap of minority states that we actually observe in the data.
            Split_Permute_Value = Reference_Gene_Minority_Group_Overlaps
            # Identify Split Direction (whether the observed arrangment is sorting towards the global minimum entropy or not. I.e. is the QF sorting into the
            # minority or majority group of the RF.)
            Sort_Out_Of_Inds = np.where((Split_Permute_Value - Max_Entropy_Permutation) < 0)[0]
            # Assign Split Directions for each QF/RF pair to a vector.
            Split_Directions = np.repeat(1,Permutables.shape[0])
            Split_Directions[Sort_Out_Of_Inds] = -1
            #
            if Observe_Directionality == 0:
                Scenario_Inds = np.where(Minority_Group_Cardinality <= Permutables)[0]
                if Scenario_Inds.shape[0] > 0:
                    Sort_Gains_Temp, Sort_Weights_Temp = Calculate_All_Fixed_RF_Sort_Values(Split_Directions[Scenario_Inds],Max_Entropy_Permutation[Scenario_Inds],Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables[Scenario_Inds],Min_Entropy_ID_1[Scenario_Inds],Min_Entropy_ID_2[Scenario_Inds],Split_Permute_Value[Scenario_Inds])
                    Sort_Gains = np.zeros(Permutables.shape[0])
                    Sort_Gains[Scenario_Inds] = Sort_Gains_Temp
                    Sort_Gains[Scenario_Inds] = Sort_Gains[Scenario_Inds] * Split_Directions[Scenario_Inds]
                    Sort_Weights = np.zeros(Permutables.shape[0])
                    Sort_Weights[Scenario_Inds] = Sort_Weights_Temp
                else:
                    Sort_Gains = np.zeros(Reference_Gene_Minority_Group_Overlaps.shape[0])
                    Sort_Weights = np.zeros(Reference_Gene_Minority_Group_Overlaps.shape[0])
            else:
                Sort_Gains, Sort_Weights = Calculate_All_Fixed_RF_Sort_Values(Split_Directions,Max_Entropy_Permutation,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables,Min_Entropy_ID_1,Min_Entropy_ID_2,Split_Permute_Value)
                Sort_Gains = Sort_Gains * Split_Directions
        else:
            Sort_Gains = np.zeros(Reference_Gene_Minority_Group_Overlaps.shape[0])
            Sort_Weights = np.zeros(Reference_Gene_Minority_Group_Overlaps.shape[0])
        Results.append(Sort_Gains)
        Results.append(Sort_Weights)
    return Results


def Calculate_All_Fixed_RF_Sort_Values(Split_Directions,Max_Entropy_Permutation,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables,Min_Entropy_ID_1,Min_Entropy_ID_2,Split_Permute_Value):
    # Calculate critical points on the ES curve
    Max_Permuation_Entropies = Calc_RF_Entropies(Max_Entropy_Permutation,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables)
    Sort_Into_Inds = np.where(Split_Directions == 1)[0]
    Sort_Out_Of_Inds = np.where(Split_Directions == -1)[0]
    Minimum_Entropies = np.zeros(Permutables.shape[0])
    if Sort_Into_Inds.shape[0] > 0:
        # The minimum entropy if none of the QF minority states are in the RF minority group.
        Sort_Into_Minimum_Entropies = Calc_RF_Entropies(Min_Entropy_ID_2[Sort_Into_Inds],Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables[Sort_Into_Inds])
        Minimum_Entropies[Sort_Into_Inds] = Sort_Into_Minimum_Entropies
    if Sort_Out_Of_Inds.shape[0] > 0:
        # The minimum entropy if the RF minority group has as many of the QF minority state samples in it as possible.
        Sort_Out_Of_Minimum_Entropies = Calc_RF_Entropies(Min_Entropy_ID_1[Sort_Out_Of_Inds],Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables[Sort_Out_Of_Inds])  
        Minimum_Entropies[Sort_Out_Of_Inds] = Sort_Out_Of_Minimum_Entropies       
    # The entropy of the arrangment observed in the data set.
    Split_Permute_Entropies = Calc_RF_Entropies(Split_Permute_Value,Minority_Group_Cardinality,Majority_Group_Cardinality,Permutables)
    # Calculate ES parabola properties
    Max_Entropy_Differences = Max_Permuation_Entropies - Minimum_Entropies
    Entropy_Losses = Max_Permuation_Entropies - Split_Permute_Entropies
    # Vector of Information Gain values for each RF/QF pair.
    Sort_Gains = Entropy_Losses/Max_Entropy_Differences
    # Vector of Split Weights values for each RF/QF pair.
    Sort_Weights = (Max_Permuation_Entropies - Minimum_Entropies) / Max_Permuation_Entropies
    return Sort_Gains, Sort_Weights

###

## Optimise the Thresholds by reducing the error of discrete states between the original data set and the converged structure ##

def Parallel_Optimise_Discretisation_Thresholds(Original_Data,Binarised_Input_Matrix,Cycle_Suggested_Imputations,Use_Cores=-1,Auto_Save=1):
     # Set number of cores to use
    Cores_Available = multiprocess.cpu_count()
    if Use_Cores == -1:
        Use_Cores = Cores_Available - 1 # -1 Is an arbitrary buffer of idle cores that I set.
        if Use_Cores < 1:
            Use_Cores = 1
    print("Cores Avaiblable: " + str(Cores_Available))
    print("Cores Used: " + str(Use_Cores))
    Binarised_Input_Matrix[Cycle_Suggested_Imputations] = (Binarised_Input_Matrix[Cycle_Suggested_Imputations] * -1) +1 
    # Only use features where imputation actually occoured
    Use = np.unique(Cycle_Suggested_Imputations[1])
    Paired = [[]] * Use.shape[0]
    for i in np.arange(Use.shape[0]):
        Paired[i] = np.stack((Original_Data[:,Use[i]],Binarised_Input_Matrix[:,Use[i]]))
    pool = multiprocess.Pool(processes = Use_Cores)
    Result = pool.map(Optimise_Discretisation_Thresholds, Paired)
    pool.close()
    pool.join()
    Optimal_Threshold = np.repeat(-1,Binarised_Input_Matrix.shape[1]).astype("f")
    Optimal_Threshold[Use] = np.asarray(Result)
    Result = np.asarray(Result,dtype=object)
    if Auto_Save == 1:
        np.save("Optimised_Thresholds.npy",Optimal_Threshold)
    return Optimal_Threshold

def Optimise_Discretisation_Thresholds(Paired):
    Original_Gene = Paired[0,:]
    Imputed_Gene = Paired[1,:]
    Unique_Exspression = np.unique(Original_Gene)
    Min_Error = np.inf
    for Thresh in np.arange(Unique_Exspression.shape[0]):
        Threshold = Unique_Exspression[Thresh]
        Threshold_Expression_States = np.zeros(Original_Gene.shape[0])
        Active_Inds = np.where(Original_Gene >= Threshold)[0]
        Threshold_Expression_States[Active_Inds] = 1
        Differences = np.absolute(Imputed_Gene-Threshold_Expression_States)
        Error = np.sum(Differences)
        if Error < Min_Error:
            Min_Error = Error
            Optimal_Threshold = Unique_Exspression[Thresh]
    return Optimal_Threshold


## Get the Information Gain and Split Weights for each gene in the dataset, for a subset of cells in the data.
# Main use is to find which gene exists in a specific region of an embedding.

def Gene_Overlays_With_Cell_Subset(Cell_IDs, Binarised_Input_Matrix, Min_Clust_Size = 10):
    Cell_Inds = np.where(np.isin(Binarised_Input_Matrix.index,Cell_IDs))[0]
    Cell_Vector = np.zeros(Binarised_Input_Matrix.shape[0]).astype("i")
    Cell_Vector[Cell_Inds] = 1
    Binarised_Input_Matrix["Cell_Subset"] = Cell_Vector
    Cell_Cardinality = Binarised_Input_Matrix.shape[0]
    global Minority_Group_Matrix
    Minority_Group_Matrix = np.asarray(Binarised_Input_Matrix)
    # Create Minority_Group_Matrix objects, Permutables and Switch_State_Inidicies objects.
    Permutables, Switch_State_Inidicies = Find_Permutations(Minority_Group_Matrix,Cell_Cardinality)
    Permutables[Permutables < Min_Clust_Size] = np.nan
    # Switch Minority/Majority states to 0/1 where necessary.
    Minority_Group_Matrix[:,Switch_State_Inidicies] = (Minority_Group_Matrix[:,Switch_State_Inidicies] * -1) + 1  
    # Find minority state overlaps with cell subset
    Overlaps = Find_Minority_Group_Overlaps(-1)
    Pass_Info_To_Cores = np.append(-1,Overlaps)
    Results = Calculate_ES_Matricies(Pass_Info_To_Cores,Cell_Cardinality,Permutables,Observe_Directionality=1)
    Sort_Gains = Results[0]
    Sort_Gains[np.isnan(Sort_Gains)] = 0
    Sort_Gains[np.isinf(Sort_Gains)] = 0
    # Retreive Positive_EP_Inds and put the features back in the original feature ordering.
    Sort_Weights = Results[1]
    Sort_Weights[np.isnan(Sort_Weights)] = 0
    Sort_Weights[np.isinf(Sort_Weights)] = 0
    return Sort_Gains, Sort_Weights


